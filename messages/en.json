{
  "header": {
    "title": "Xinference",
    "nav": {
      "features": "Features",
      "models": "Models",
      "docs": "Documentation",
      "contact": "Contact Us"
    },
    "cta": {
      "enterprise": "Enterprise",
      "compare": "Compare",
      "partner": "Partner"
    }
  },
  "hero": {
    "title": "Xinference: Unified AI Inference.",
    "subtitle": "Any Model, Any Hardware, Peak Performance.",
    "description": "Effortlessly deploy, manage, and scale LLMs, multimodal, and speech models with our enterprise-grade, full-stack inference platform. Your infrastructure, your models, optimized.",
    "features": "Model lifecycle management, multiple inference engines, heterogeneous GPUs, high throughput and availability, efficient scheduling"
  },
  "features": {
    "models": {
      "title": "Rich Model Support",
      "description": "100+ latest open-source models from text, speech and video to embedding/rerank models, with fastest adaptation."
    },
    "hardware": {
      "title": "Wide Hardware Support",
      "description": "Support for multiple hardware platforms, including domestic GPUs like Huawei Ascend, Hygon, and Tianshu. Can support multiple types of hardware serving together."
    },
    "ecosystem": {
      "title": "Rich Ecosystem",
      "description": "Multiple mainstream development frameworks natively support Xinference, including Langchain, Dify, Ragflow, FastGPT, etc."
    },
    "engines": {
      "title": "Multiple Inference Engines",
      "description": "Optimized support for multiple mainstream inference engines, including vLLM, SGLang, TensorRT, Transformers, MLX, LMDeploy, etc."
    },
    "performance": {
      "title": "High Performance Inference",
      "description": "Native distributed architecture, easily horizontally scalable clusters, support for multiple scheduling strategies adapted to low latency, high context, high throughput and other scenarios."
    },
    "enterprise": {
      "title": "Enterprise Features",
      "description": "User management, SSO, batch processing, multi-tenant isolation, model fine-tuning, observability, and many more enterprise-grade features."
    }
  },
  "openSource": {
    "title": "Open Source & Free to Use",
    "description": "Xinference is an open-source project that's free to use, customize, and extend according to your needs."
  },
  "github": {
    "star": "Star on GitHub"
  },
  "footer": {
    "copyright": "Copyright © 2024 Hangzhou Future Speed Technology Co., Ltd.",
    "icp": "浙ICP备2022033013号",
    "security": "浙公网安备：33011002016954"
  }
}