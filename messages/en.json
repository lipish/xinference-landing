{
  "header": {
    "title": "Xinference",
    "nav": {
      "features": "Features",
      "pricing": "Pricing",
      "docs": "Documentation",
      "contact": "Contact Us"
    },
    "cta": {
      "enterprise": "Enterprise",
      "compare": "Compare",
      "partner": "Partner"
    }
  },
  "hero": {
    "title": "Unified AI Inference",
    "subtitle": "Any Model, Any Hardware, Peak Performance.",
    "description": "Effortlessly deploy, manage, and scale LLMs, multimodal, and speech models with our enterprise-grade, full-stack inference platform. Your infrastructure, your models, optimized.",
    "features": "Model lifecycle management, multiple inference engines, heterogeneous GPUs, high throughput and availability, efficient scheduling"
  },
  "features": {
    "engines": {
      "title": "Multiple Inference Engines",
      "description": "Optimized support for multiple mainstream inference engines, including vLLM, SGLang, TensorRT, Transformers, MLX, LMDeploy, etc."
    },
    "hardware": {
      "title": "Wide Hardware Support",
      "description": "Support for multiple hardware platforms, including domestic GPUs like Huawei Ascend, Hygon, and Tianshu. Can support multiple types of hardware serving together."
    },
    "performance": {
      "title": "Enterprise Distributed Deployment",
      "description": "Based on the self-developed Xoscar high-performance distributed computing foundation, it supports stable operation at a scale of 200,000 cores and features automatic load balancing and fault recovery capabilities."
    },
    "models": {
      "title": "Rich Model Support",
      "description": "100+ latest open-source models from text, speech and video to embedding/rerank models, with fastest adaptation."
    },
    "enterprise": {
      "title": "Enterprise Features",
      "description": "User management, SSO, batch processing, multi-tenant isolation, model fine-tuning, observability, and many more enterprise-grade features."
    },
    "ecosystem": {
      "title": "Rich Ecosystem",
      "description": "Seamlessly integrate with popular AI frameworks and tools. Compatible with OpenAI API, LangChain, and other mainstream AI development ecosystems."
    },
    "concurrency": {
      "title": "High Concurrency Optimization",
      "description": "Optimized for enterprise high-concurrency scenarios, supports structured output, provides memory optimization and performance acceleration, ensuring business continuity and stability."
    }
  },
  "openSource": {
    "title": "Open Source & Free to Use",
    "description": "Xinference is an open-source project that's free to use, customize, and extend according to your needs."
  },
  "github": {
    "star": "Star on GitHub"
  },
  "footer": {
    "copyright": "Copyright © 2024 Hangzhou Future Speed Technology Co., Ltd.",
    "icp": "浙ICP备2022033013号",
    "security": "浙公网安备：33011002016954"
  }
}